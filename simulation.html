<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Simulation</title>
    <link
      href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2/dist/tailwind.min.css"
      rel="stylesheet"
      type="text/css"
    />
  </head>

  <body>
    <style>
      canvas {
        padding-left: 0;
        padding-right: 0;
        margin-left: auto;
        margin-right: auto;
        display: block;
        width: 400px;
      }
    </style>
    <nav>
      <div
        class="container flex items-center justify-between px-6 py-3 mx-auto"
      >
        <div>
          <a
            class="text-2xl font-bold text-gray-800 dark:text-white lg:text-3xl hover:text-gray-700 dark:hover:text-gray-300"
            href="#"
            >Emo Detector</a
          >
        </div>

        <a
          class="my-1 text-sm font-medium text-gray-700 dark:text-gray-200 hover:text-blue-500 dark:hover:text-blue-400 lg:mx-4 lg:my-0"
          href="#"
        >
          <svg
            xmlns="http://www.w3.org/2000/svg"
            class="w-6 h-6"
            viewBox="0 0 20 20"
            fill="currentColor"
          >
            <path
              fill-rule="evenodd"
              d="M18 10a8 8 0 11-16 0 8 8 0 0116 0zm-8-3a1 1 0 00-.867.5 1 1 0 11-1.731-1A3 3 0 0113 8a3.001 3.001 0 01-2 2.83V11a1 1 0 11-2 0v-1a1 1 0 011-1 1 1 0 100-2zm0 8a1 1 0 100-2 1 1 0 000 2z"
              clip-rule="evenodd"
            />
          </svg>
        </a>
      </div>
    </nav>
    <div class="container px-auto py-16 mx-auto text-center">
      <div>
        <h1
          class="text-2xl font-bold text-slate-800 dark:text-white md:text-2xl mb-2"
        >
          We've got you ðŸ˜‰
        </h1>
        <h3>Start detecting emotions</h3>
        <button
          onclick="init()"
          class="px-4 py-2 mt-8 font-medium tracking-wide text-white capitalize transition-colors duration-200 transform bg-blue-600 rounded-md hover:bg-blue-500 focus:outline-none focus:ring focus:ring-blue-300 focus:ring-opacity-80"
        >
          Fire up the Camera!ðŸ“¸
        </button>
        <div class="mt-5">
          <div id="webcam-container"></div>
          <div
            id="label-container"
            class="w-full bg-gray-200 rounded-full"
          ></div>
        </div>
      </div>
    </div>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.3.1/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@teachablemachine/image@0.8/dist/teachablemachine-image.min.js"></script>
    <script type="text/javascript">
      // More API functions here:
      // https://github.com/googlecreativelab/teachablemachine-community/tree/master/libraries/image

      // the link to your model provided by Teachable Machine export panel
      const URL = "https://teachablemachine.withgoogle.com/models/YyQBsb8El/";

      let model,
        webcam,
        labelContainer,
        maxPredictions,
        levels,
        prediction;

      // Load the image model and setup the webcam
      async function init() {
        const modelURL = URL + "model.json";
        const metadataURL = URL + "metadata.json";

        // load the model and metadata
        // Refer to tmImage.loadFromFiles() in the API to support files from a file picker
        // or files from your local hard drive
        // Note: the pose library adds "tmImage" object to your window (window.tmImage)
        model = await tmImage.load(modelURL, metadataURL);
        maxPredictions = model.getTotalClasses();

        // Convenience function to setup a webcam
        const flip = true; // whether to flip the webcam
        webcam = new tmImage.Webcam(200, 200, flip); // width, height, flip
        await webcam.setup(); // request access to the webcam
        await webcam.play();
        window.requestAnimationFrame(loop);

        // append elements to the DOM
        document.getElementById("webcam-container").appendChild(webcam.canvas);
        labelContainer = document.getElementById("label-container");
        let pre =await model.predict(webcam.canvas)
        for (let i = 0; i < maxPredictions; i++) {
          // and class labels
          levels = document.createElement("div");
          levels.classList.add(
            "my-2",
            // "bg-blue-600",
            "text-xs",
            "font-medium",
            "text-blue-100",
            "text-left",
            "p-1",
            "leading-none",
            "rounded-l-full",
            "bg-green-500"
          );
          levels.style.width = `100%`;
          console.log(prediction)
          labelContainer.appendChild(levels);
        }
      }

      async function loop() {
        webcam.update(); // update the webcam frame
        await predict();
        window.requestAnimationFrame(loop);
      }

      // run the webcam image through the image model
      async function predict() {
        // predict can take in an image, video or canvas html element
         prediction = await model.predict(webcam.canvas);

        for (let i = 0; i < maxPredictions; i++) {

          const classPrediction =
            prediction[i].className +
            " : " +
            prediction[i].probability.toFixed(2) * 100 +
            "%";
          labelContainer.childNodes[i].innerHTML = classPrediction;

        }
      }
    </script>
  </body>
</html>
